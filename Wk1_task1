%%writefile wc_mapper.py
import re
import sys

def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

for line in sys.stdin:
    try:
        article_id, text = line.strip().split('\t', 1)
    except ValueError as e:
        continue
    words = re.split("\W*\s+\W*", text, flags=re.UNICODE)
    for word in words:
        eprint("reporter:counter:Wiki stats,Total words,%d" % 1)
        print("%s\t%d" % (word.lower(), 1)) 
        
%%writefile wc_reducer.py

import sys

current_idx = None
word_sum = 0

%%writefile -a wc_reducer.py

for line in sys.stdin:
    try:
        idx, count = line.strip().split('\t', 1)
        count = int(count)
    except ValueError as e:
        continue
    if current_idx != key:
        if current_idx:
            print("%s\t%d" % (current_idx, word_sum))
        word_sum = 0
        current_idx = idx
    word_sum += count

if current_idx:
    print("%s\t%d" % (current_idx, word_sum))
    
! hdfs dfs -ls /data/

%%bash

OUT_DIR="wordcount_result_"$(date +"%s%6N")
NUM_REDUCERS=8  # Do not change it

hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapreduce.job.name="Streaming Word Count" \
    -D mapreduce.job.reduces=${NUM_REDUCERS} \
    -files wc_mapper.py,wc_reducer.py \
    -mapper "python3 wc_mapper.py" \
    -combiner "python3 wc_reducer.py" \
    -reducer "python3 wc_reducer.py" \
    -input /data/wiki/en_articles_part \
    -output ${OUT_DIR} > /dev/null

hdfs dfs -cat ${OUT_DIR}/part-00000 | head 

